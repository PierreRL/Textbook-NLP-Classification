{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pierrelardet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pierrelardet/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "np.random.seed(1000)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification of Textbooks\n",
    "### Pierre Lardet\n",
    "\n",
    "This code is presented as a python notebook, using python 3.11.2. My thoughts are presented chronologically\n",
    "\n",
    "Versions of libraries used are listed below.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Before anything else, we need to be able to read in the text and convert it into a format which is easy to manipulate. I'm going to use a [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). I noticed that the Computer Science text files were nested in an extra directory so manually moved them to make the structure of text files consistent. Next, I read in all of the text files in an easily extensible manner, stored them in a 2d array labelled with their subject and created a new Pandas dataframe to make further manipulation easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['Computer_Science', 'History', 'Maths']\n",
    "raw_texts = []\n",
    "\n",
    "def read_subject_data(subject:str)->None:\n",
    "    for dir in glob.glob(f'./data/{subject}/*.txt'):\n",
    "        f = open(dir, 'r')\n",
    "        text = f.read()\n",
    "        raw_texts.append([subject, text, dir])\n",
    "        f.close()\n",
    "\n",
    "for subject in subjects:\n",
    "    read_subject_data(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Sample of the data: \n",
      "             subject                                               text   \n",
      "0  Computer_Science  4.8 Exercises 275\\n4.15 [IS) <§§4.2, 4.3> One ...  \\\n",
      "1  Computer_Science  4.5 Fallacies and Pitfalls 26.\\nFirst we find ...   \n",
      "2  Computer_Science  518 Chapter 7 Large and Fast: Exploiting Memor...   \n",
      "3  Computer_Science  Computers\\nReconstructing the\\nin the\\nAncient...   \n",
      "4  Computer_Science  230 Chapter 3 Arithmetic: for Computers\\n3.9 [...   \n",
      "\n",
      "                                               dir  \n",
      "0  ./data/Computer_Science/Computer_Science291.txt  \n",
      "1  ./data/Computer_Science/Computer_Science285.txt  \n",
      "2  ./data/Computer_Science/Computer_Science534.txt  \n",
      "3  ./data/Computer_Science/Computer_Science252.txt  \n",
      "4  ./data/Computer_Science/Computer_Science246.txt  \n",
      "--------------------------------------------------\n",
      "Dimensions: (1341, 3)\n",
      "--------------------------------------------------\n",
      "Counts of each subject: \n",
      "                   text  dir\n",
      "subject                    \n",
      "Computer_Science   637  637\n",
      "History            490  490\n",
      "Maths              214  214\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "textbooks = pd.DataFrame(raw_texts, columns=['subject', 'text' ,'dir'])\n",
    "\n",
    "textbooks = textbooks[textbooks['text'] != '']\n",
    "\n",
    "print('-'*50)\n",
    "print(f'Sample of the data: \\n {textbooks.head()}')\n",
    "print('-'*50)\n",
    "print(f'Dimensions: {textbooks.shape}')\n",
    "print('-'*50)\n",
    "print(f'Counts of each subject: \\n {textbooks.groupby(\"subject\").count()}')\n",
    "print('-'*50)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataframe to work with. The text is currently very messy with lots of extra characters and spacing etc. In order to use the text as an input into a ML classification model, it needs to be much cleaner. The desired format will be a list of lower-case words in each sample which can later be converted to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            subject                                               text   \n",
      "0  Computer_Science  4.8 Exercises 275\\n4.15 [IS) <§§4.2, 4.3> One ...  \\\n",
      "1  Computer_Science  4.5 Fallacies and Pitfalls 26.\\nFirst we find ...   \n",
      "2  Computer_Science  518 Chapter 7 Large and Fast: Exploiting Memor...   \n",
      "3  Computer_Science  Computers\\nReconstructing the\\nin the\\nAncient...   \n",
      "4  Computer_Science  230 Chapter 3 Arithmetic: for Computers\\n3.9 [...   \n",
      "\n",
      "                                               dir   \n",
      "0  ./data/Computer_Science/Computer_Science291.txt  \\\n",
      "1  ./data/Computer_Science/Computer_Science285.txt   \n",
      "2  ./data/Computer_Science/Computer_Science534.txt   \n",
      "3  ./data/Computer_Science/Computer_Science252.txt   \n",
      "4  ./data/Computer_Science/Computer_Science246.txt   \n",
      "\n",
      "                                          text_clean  \n",
      "0   exercise one user told three program exercise...  \n",
      "1   fallacy pitfall first find execution time two...  \n",
      "2   chapter large fast exploiting memory hierarch...  \n",
      "3  computer reconstructing ancient world real wor...  \n",
      "4   chapter arithmetic computer ioj bit address t...  \n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lem = WordNetLemmatizer()\n",
    "in_stopwords = lambda word: word not in stopwords\n",
    "\n",
    "def clean_text(str:str) -> str:\n",
    "    str = re.sub(r'\\n', ' ', str)\n",
    "    str = re.sub(r'[^a-zA-Z]+', ' ', str)\n",
    "    str = str.lower()\n",
    "\n",
    "    lst_words = str.split(' ')\n",
    "\n",
    "    lst_words = list(filter(in_stopwords, lst_words))\n",
    "    lst_words = [lem.lemmatize(word) for word in lst_words]\n",
    "\n",
    "    return ' '.join(lst_words)\n",
    "\n",
    "textbooks['text_clean'] = textbooks['text'].apply(clean_text)\n",
    "\n",
    "print(textbooks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26900\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(textbooks['text_clean'], textbooks['subject'], test_size=0.3, random_state=4)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "train_y = Encoder.fit_transform(train_y)\n",
    "test_y = Encoder.fit_transform(test_y)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "Tfidf_vect.fit(textbooks['text_clean'])\n",
    "\n",
    "train_x = Tfidf_vect.transform(train_x)\n",
    "test_x = Tfidf_vect.transform(test_x)\n",
    "\n",
    "selector = SelectKBest(chi2, k = 26900)\n",
    "train_x = selector.fit_transform(train_x, train_y)\n",
    "test_x = selector.fit_transform(test_x, test_y)\n",
    "\n",
    "print(train_x.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9950372208436724\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "WRITEBACKIFCOPY base is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m predictions \u001b[39m=\u001b[39m SVM\u001b[39m.\u001b[39mpredict(test_x)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(accuracy_score(test_y, predictions))\n\u001b[0;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(SVM\u001b[39m.\u001b[39;49mcoef_\u001b[39m.\u001b[39;49mmax())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/scipy/sparse/_data.py:324\u001b[0m, in \u001b[0;36m_minmax_mixin.max\u001b[0;34m(self, axis, out)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    295\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39m    Return the maximum of the matrix or maximum along an axis.\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    This takes all elements into account, not just the non-zero ones.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m \n\u001b[1;32m    323\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_min_or_max(axis, out, np\u001b[39m.\u001b[39;49mmaximum)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/scipy/sparse/_data.py:205\u001b[0m, in \u001b[0;36m_minmax_mixin._min_or_max\u001b[0;34m(self, axis, out, min_or_max)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnnz \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m zero\n\u001b[0;32m--> 205\u001b[0m m \u001b[39m=\u001b[39m min_or_max\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_deduped_data()\u001b[39m.\u001b[39mravel())\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnnz \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mprod(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape):\n\u001b[1;32m    207\u001b[0m     m \u001b[39m=\u001b[39m min_or_max(zero, m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/scipy/sparse/_data.py:32\u001b[0m, in \u001b[0;36m_data_matrix._deduped_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_deduped_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msum_duplicates\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msum_duplicates()\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/scipy/sparse/_compressed.py:1118\u001b[0m, in \u001b[0;36m_cs_matrix.sum_duplicates\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_canonical_format:\n\u001b[1;32m   1117\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 1118\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msort_indices()\n\u001b[1;32m   1120\u001b[0m M, N \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n\u001b[1;32m   1121\u001b[0m _sparsetools\u001b[39m.\u001b[39mcsr_sum_duplicates(M, N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices,\n\u001b[1;32m   1122\u001b[0m                                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/scipy/sparse/_compressed.py:1164\u001b[0m, in \u001b[0;36m_cs_matrix.sort_indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sort the indices of this matrix *in place*\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_sorted_indices:\n\u001b[0;32m-> 1164\u001b[0m     _sparsetools\u001b[39m.\u001b[39;49mcsr_sort_indices(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr,\n\u001b[1;32m   1165\u001b[0m                                   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m   1166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_sorted_indices \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: WRITEBACKIFCOPY base is read-only"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', gamma='auto')\n",
    "SVM.fit(train_x, train_y)\n",
    "predictions = SVM.predict(test_x)\n",
    "print(accuracy_score(test_y, predictions))\n",
    "print(SVM.coef_.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
